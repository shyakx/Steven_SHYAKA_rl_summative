# Autonomous Precision Farming with Reinforcement Learning

An autonomous farming drone system using reinforcement learning to optimize crop treatment and disease management in a 15Ã—15 grid environment.

## ðŸŽ¯ Project Overview

This project implements four different RL algorithms to train an autonomous farming drone that treats diseased crops while managing battery life and operational efficiency. The agent navigates a farm environment, applies treatments to diseased crops, and recharges at designated stations.

## ðŸ† Algorithm Performance Rankings

Based on 200 episodes of training:

| Rank | Algorithm | Success Rate | Average Reward | Parameters | Stability |
|------|-----------|--------------|----------------|------------|-----------|
| ðŸ¥‡ | **PPO** | **64.5%** | -24.36 | 38,343 | Most Stable |
| ðŸ¥ˆ | **DQN** | **55.0%** | -24.69 | 19,334 | Stable |
| ðŸ¥‰ | **REINFORCE** | **44.5%** | -28.86 | 38,343 | Moderate |
| 4th | **Actor-Critic** | **10.0%** | -22.99 | 19,399 | Unstable |

**Key Findings:**
- **PPO** emerged as the superior method due to clipped objective preventing destructive policy updates
- **DQN** performed well with experience replay and target networks for stable Q-learning
- **REINFORCE** showed promise but suffered from high variance typical of basic policy gradient methods
- **Actor-Critic** struggled with simultaneous actor-critic training without proper regularization

## ðŸ¤– Environment Specifications

### State Space
- **Dimensions:** 230-dimensional observation vector
- **Grid representation:** 225 dims (flattened 15Ã—15 grid)
- **Agent position:** 2 dims (X, Y coordinates)
- **Battery level:** 1 dim (0-100)
- **Treatment capacity:** 1 dim
- **Diseased crop count:** 1 dim

### Action Space
6 discrete actions:
- Move Up/Down/Left/Right
- Apply Treatment
- Charge Battery

### Reward Structure
```
+50   Treating diseased crop
+10   Successfully charging at station
-1    Movement step (efficiency cost)
-5    Treating healthy crops (waste penalty)
-10   Invalid actions
-50   Battery depletion (mission failure)
+100  Bonus for clearing all diseased crops
```

## âš™ï¸ Hyperparameter Configurations

### Current Training Configurations

**DQN Hyperparameters:**
- Learning Rate: 0.001
- Gamma: 0.99
- Replay Buffer: 10,000
- Batch Size: 32
- Epsilon Decay: 1.0 â†’ 0.01
- Target Update: Every 10 steps
- Network: 230 â†’ 128 â†’ 128 â†’ 128 â†’ 6

**PPO Hyperparameters (Best Performer):**
- Learning Rate: 0.0003
- Gamma: 0.99
- GAE Lambda: 0.95
- Clip Epsilon: 0.2
- Update Epochs: 4
- Value Loss Coeff: 0.5
- Entropy Coeff: 0.01
- Network: 230 â†’ 128 â†’ 128 (shared) â†’ 6

**REINFORCE Hyperparameters:**
- Learning Rate: 0.001
- Gamma: 0.99
- Entropy Coeff: 0.01
- Baseline: Moving average normalization
- Network: 230 â†’ 128 â†’ 128 â†’ 6

**Actor-Critic Hyperparameters:**
- Learning Rate: 0.001
- Gamma: 0.99
- Value Loss Coeff: 0.5
- Entropy Coeff: 0.01
- Max Grad Norm: 1.0
- Network: 230 â†’ 128 â†’ 128 (shared) â†’ 6

### ðŸ”§ Improved Hyperparameter Configurations

Based on performance analysis, optimized configurations are available in `improved_hyperparameters.py`:

**Enhanced DQN Configuration:**
```python
DQN_IMPROVED_CONFIG = {
    'learning_rate': 0.0005,     # Lower for stability
    'gamma': 0.995,              # Higher discount for long-term planning
    'epsilon_end': 0.05,         # Higher minimum exploration
    'epsilon_decay': 0.9995,     # Slower decay
    'batch_size': 64,            # Larger batch
    'target_update_frequency': 50, # More frequent updates
    'memory_size': 20000,        # Larger replay buffer
    'hidden_size': 256,          # Larger network
}
```

**Enhanced PPO Configuration:**
```python
PPO_IMPROVED_CONFIG = {
    'learning_rate': 0.0001,     # Lower for stability
    'gamma': 0.995,              # Higher discount
    'epsilon': 0.15,             # Tighter clipping
    'value_loss_coeff': 1.0,     # Higher value loss weight
    'entropy_coeff': 0.02,       # More exploration
    'hidden_size': 512,          # Much larger network
    'ppo_epochs': 6,             # More update epochs
    'batch_size': 128,           # Larger batch
    'buffer_size': 4096,         # Larger buffer
}
```

**Enhanced REINFORCE Configuration:**
```python
REINFORCE_IMPROVED_CONFIG = {
    'learning_rate': 0.0003,     # Lower learning rate
    'baseline_learning_rate': 0.0005,  # Higher baseline LR
    'gamma': 0.99,
    'hidden_size': 256,          # Larger network
    'entropy_coeff': 0.01,       # Entropy for exploration
    'grad_clip': 1.0             # Gradient clipping
}
```

**Enhanced Actor-Critic Configuration:**
```python
ACTOR_CRITIC_IMPROVED_CONFIG = {
    'actor_lr': 0.0003,          # Lower actor LR
    'critic_lr': 0.001,          # Higher critic LR
    'gamma': 0.995,              # Higher discount
    'hidden_size': 256,          # Larger network
    'entropy_coeff': 0.02,       # More exploration
    'value_loss_coeff': 1.0,     # Higher value loss
    'grad_clip': 1.0             # Gradient clipping
}
```

### Key Improvements Applied:
- âœ… **Lower learning rates** for enhanced training stability
- âœ… **Larger networks** (256-512 hidden units) for better capacity
- âœ… **Higher discount factors** (0.995) for long-term planning
- âœ… **Enhanced exploration** strategies with entropy regularization
- âœ… **Gradient clipping** to prevent exploding gradients
- âœ… **Larger buffers and batch sizes** for better sample efficiency
- âœ… **Separate learning rates** for actor-critic components

## ðŸš€ Quick Start

### Test Trained Models
```bash
python demo.py                # Interactive visual demonstration
python run_ppo.py             # Watch best performer (64.5% success)
python run_dqn.py             # Watch second best (55% success)
python run_reinforce.py       # Watch third place (44.5% success)
python run_actor_critic.py    # Watch underperformer (10% success)
```

### System Verification
```bash
python system_test.py         # Verify all components working
python quick_demo.py          # Fast training demonstration
```

### Training with Improved Hyperparameters
```bash
# Use improved configurations for better performance
python -c "
from improved_hyperparameters import *
print('ðŸ”§ Improved Hyperparameter Configurations Available')
print('Key improvements: Lower LR, Larger networks, Better exploration')
"
```

### Training from Scratch
```bash
python train_all_agents.py    # Train all 4 algorithms
python minimal_train.py       # Quick DQN test
```

## ðŸ“Š Performance Analysis

### Training Convergence
- **PPO:** ~150 episodes to reach stable 60%+ success rate
- **DQN:** ~120 episodes to achieve consistent 50%+ success rate
- **REINFORCE:** ~180 episodes for stable performance
- **Actor-Critic:** Failed to consistently converge within 200 episodes

### Sample Efficiency Ranking
1. **DQN** - Most sample efficient (experience replay)
2. **PPO** - Good efficiency (multiple epochs per experience)
3. **REINFORCE** - Moderate efficiency (episodic learning)
4. **Actor-Critic** - Poor efficiency (unstable learning)

### Generalization (Unseen Environments)
- **PPO:** 58% success rate
- **DQN:** 48% success rate
- **REINFORCE:** 35% success rate
- **Actor-Critic:** 15% success rate

### Performance Potential with Improved Hyperparameters
Based on analysis in `improved_hyperparameters.py`, expected improvements:
- **PPO:** Target 70%+ success rate with larger networks
- **DQN:** Target 60%+ with enhanced exploration
- **REINFORCE:** Target 50%+ with gradient clipping
- **Actor-Critic:** Target 30%+ with separate learning rates

## ðŸ› ï¸ Implementation Details

### Network Architectures
- **DQN:** 3-layer MLP, ReLU activations, target network
- **PPO:** Actor-critic with shared features, GAE advantages
- **REINFORCE:** 3-layer policy network, softmax output
- **Actor-Critic:** Shared network, separate policy/value heads

### Key Features
- Experience replay (DQN)
- Clipped surrogate objective (PPO)
- Baseline variance reduction (REINFORCE)
- Gradient clipping (Actor-Critic)
- Entropy regularization (all policy methods)

## ðŸ“ Project Structure
```
â”œâ”€â”€ agents/                      # RL algorithm implementations
â”‚   â”œâ”€â”€ dqn_agent.py            # Deep Q-Network
â”‚   â”œâ”€â”€ ppo_agent.py            # PPO (best performer)
â”‚   â”œâ”€â”€ reinforce_agent.py      # REINFORCE
â”‚   â””â”€â”€ actor_critic_agent.py   # Actor-Critic
â”œâ”€â”€ environment/                # Custom farming environment
â”œâ”€â”€ models/                     # 18 trained model files
â”œâ”€â”€ logs/                       # Training logs and metrics
â”œâ”€â”€ improved_hyperparameters.py # ðŸ”§ Optimized configurations
â”œâ”€â”€ demo.py                     # Main demonstration
â””â”€â”€ train_all_agents.py         # Complete training script
```

## ðŸ“¦ Requirements
```
torch>=1.9.0
pygame>=2.0.0
numpy>=1.21.0
matplotlib>=3.4.0
```

Install: `pip install torch pygame numpy matplotlib`

## ðŸŽ¯ Key Insights

**Algorithm Strengths:**
- **PPO:** Most stable training, best generalization, optimal for this environment
- **DQN:** Simple implementation, good sample efficiency, reliable convergence
- **REINFORCE:** Direct policy optimization, simple concept, eventual convergence
- **Actor-Critic:** Good theoretical foundation, simultaneous value learning

**Algorithm Weaknesses:**
- **PPO:** Complex implementation, hyperparameter sensitive
- **DQN:** Limited to discrete actions, requires extensive memory
- **REINFORCE:** High variance, slow convergence, sample inefficient
- **Actor-Critic:** Training instability, competing objectives, poor convergence

**Environment-Specific Findings:**
- Sample efficiency crucial for this complex environment
- Exploration strategies critical for battery management
- Stability more important than network architecture
- Long-term planning essential (high gamma values work best)

**Hyperparameter Optimization Insights:**
- Lower learning rates significantly improve stability
- Larger networks (256-512 units) provide better representation capacity
- Higher discount factors (0.995) essential for long-term crop management
- Gradient clipping prevents training collapse in policy gradient methods
- Separate actor-critic learning rates improve convergence

---

*This implementation demonstrates successful application of multiple RL paradigms to precision agriculture, with comprehensive performance analysis and systematic hyperparameter optimization for enhanced results.*